{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44988ae2-bec2-498e-9a76-7b4f29626262",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ‚≠ê"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d491d-d7a9-4515-972b-0e43fa76cddc",
   "metadata": {},
   "source": [
    "### üî¥ 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f20b6e5-9ea4-43cc-9cbf-2b0b58fc303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FileVideoStream\n",
    "import time\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33335a1d-e08a-41ab-b6f7-bedac0e2b55c",
   "metadata": {},
   "source": [
    "### üî¥ 2. Load Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "364bb633-5e5b-4a0b-8aeb-529cffdb334d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at mask_classifier_mobilenet.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmask_classifier_mobilenet.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\saving\\save.py:206\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    205\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(filepath_str, \u001b[38;5;28mcompile\u001b[39m, options)\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at mask_classifier_mobilenet.h5"
     ]
    }
   ],
   "source": [
    "classifier = tf.keras.models.load_model('mask_classifier_mobilenet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b0fe5-f733-4f1e-8f84-ec503c19c687",
   "metadata": {},
   "source": [
    "### üî¥ 3. Load Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85b5af-bde8-4468-91ce-fb323267eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxtPath = r\"face_detector_ssd_caffee/deploy.prototxt\" # configurations file \n",
    "weightsPath = r\"face_detector_ssd_caffee/res10_300x300_ssd_iter_140000.caffemodel\" # weights file\n",
    "detector = cv2.dnn.readNet(prototxtPath, weightsPath) # load saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a75350-14a0-46d5-b567-c772a045308a",
   "metadata": {},
   "source": [
    "### üî¥ 4. Detect Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7776161-e9f5-4e77-ba99-32ac8dd207a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_and_classify(frame, detector, classifier, conf = 0.3):\n",
    "    '''\n",
    "    Input: video frame, face detection model, face mask classification model\n",
    "    Output: detector bounding boxes, classifier predictions, bounding boxes for tracker\n",
    "    \n",
    "    detect faces in frame and perform mask classification\n",
    "    turn frame into blob (essentially preprocessing: 1. mean subtraction, 2. scaling, 3. optionally channel swapping)\n",
    "    '''\n",
    "    \n",
    "    RGB = (104.0, 177.0, 123.0) # Source and intuition: \"Deep Learning and Mean Subtraction\": https://pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\n",
    "    pixels = 224\n",
    "    blob = cv2.dnn.blobFromImage(frame, scalefactor=1.0, size=(pixels, pixels), mean=RGB)\n",
    "    \n",
    "    ''' DETECTION '''\n",
    "    # Detect Faces\n",
    "    detector.setInput(blob)\n",
    "    detections = detector.forward()\n",
    "    num_of_detections = detections.shape[2]\n",
    "\n",
    "    # initialise list of faces, their locations, list of predictions for our mask classifier\n",
    "    faces = []\n",
    "    locs = []\n",
    "    preds = []\n",
    "    \n",
    "    # loop over all face detections\n",
    "    for i in range(num_of_detections):\n",
    "        \n",
    "        # Live Counter to display on video feed\n",
    "        num_of_masked = 0\n",
    "        num_of_unmasked = 0\n",
    "        num_of_uncertain = 0\n",
    "        \n",
    "        # extract the confidence (probability) in the detection\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the confidence is  greater than the minimum confidence\n",
    "        if confidence > conf:\n",
    "            \n",
    "            # compute the (x, y)-coordinates of the bounding box for the object \n",
    "            box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "    \n",
    "            # ensure the bounding boxes fall within the dimensions of the frame \n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(W - 1, endX), min(H - 1, endY))\n",
    "\n",
    "            # Extract Face \n",
    "            face = frame[startY:endY, startX:endX]                          # 1. extract the face region of interest (ROI) \n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)                    # 2. convert it from BGR to RGB channel ordering \n",
    "            face = cv2.resize(face, (224, 224))                             # 3. resize it to 224x224 \n",
    "            face = tf.keras.preprocessing.image.img_to_array(face)          # 4. preprocess it for the mask classifier\n",
    "            face = tf.keras.applications.mobilenet_v2.preprocess_input(face)\n",
    "\n",
    "            # append face and bounding box to lists\n",
    "            faces.append(face)\n",
    "            locs.append(np.array([startX, startY, endX, endY]))\n",
    "\n",
    "    ''' CLASSIFICATION '''\n",
    "    # only make a predictions if at least one face was detected\n",
    "    if len(faces) > 0:\n",
    "        # for faster inference make batch predictions on *all* faces at the same time rather than one-by-one predictions in the above `for` loop\n",
    "        faces = np.array(faces, dtype=\"float32\")\n",
    "        preds = classifier.predict(faces, batch_size=32, verbose=0)\n",
    "\n",
    "    return locs, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f0cd0-7db7-4698-b7b4-ff992b9804ac",
   "metadata": {},
   "source": [
    "### üî¥ 5. TrackerCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c84982-a557-46dd-8d13-019bb19e644f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from random import randrange\n",
    "\n",
    "class TrackerCounter:\n",
    "    \n",
    "    people_count = 0\n",
    "    mask_count = 0\n",
    "    nomask_count = 0\n",
    "    uncertain_count = 0\n",
    "    \n",
    "    def __init__(self):\n",
    "       \n",
    "        # Store information for each object in a dictionary\n",
    "        # values: [cx (int), cy (int), has_been_before_boundary (boolean), has_been_counted_before (boolean), list_of_up_to_10_predictions (float [-1, 1])]\n",
    "        self.center_points = {}\n",
    "        \n",
    "        # each time a new object id detected, the count will increase by one\n",
    "        self.id_count = 0\n",
    "\n",
    "\n",
    "    def update(self, objects_rect, label_probs, W, H, dist_same_obj=100):\n",
    "            \n",
    "        '''\n",
    "        object_rect: List of objects' coordinates = [(xstart1, ystart1, xend1, yend1), (xstart2, ystart2, xend2, yend2), ...]\n",
    "        label_probs: List of objects' label \"probabilities\" = [-0.9, 0.7, ....]\n",
    "        W, H: frame width and height\n",
    "        \n",
    "        dist_same_obj:\n",
    "        - It is the maximum eucledian distance from the previous object in order to be considered the same object\n",
    "        - dist_same_obj determines how far the detection has to be from the old one to be considered a new object. \n",
    "        - If dist_same_obj is too low we might get false positives when an object is moving. \n",
    "        - It also depends on the amount of pixels so needs different value when resolution changes.\n",
    "        '''\n",
    "            \n",
    "        # List of info of all objects in the frame (this is what the method returns)\n",
    "        objects_infos = []\n",
    "        \n",
    "        # NB: If dist_same_obj is too low then fast moving faces are seen as a new face in each frame and averaging classification is not done OR even worse they are not counted at all (because a new face appears after the boundary)\n",
    "        # NB: If dist_same_obj is too high then if a face disappears at frame 15 and a new face appears at frame 16 (it is only a problem if this happens in consecutive frames since memory is not implemented i.e. we got 1 frame memory) \n",
    "        # then the new face will be seen as the old face and might not be tracked\n",
    "        # NB: This is very unlikely to be an issue when memory = 1 as it is now so it is a lot safer to have a high dist_same_obj than a low one. \n",
    "        dist_same_obj=(W+H)/10\n",
    "        \n",
    "       \n",
    "        counter=0 # This variable represents how many faces of previous frame have been matched to faces of the new frame (e.g. if counter = 2 it means that 2 faces from the previous frame have been matched to 2 of the new frame). This makes sure two or more faces are not labelled the same \n",
    "        \n",
    "         # Loop through all faces and their labels\n",
    "        for rect, prob in zip(objects_rect, label_probs):\n",
    "            xstart, ystart, xend, yend = rect\n",
    "            # Get center point of new object\n",
    "            cx = (xstart + xend) // 2\n",
    "            cy = (ystart + yend) // 2 \n",
    "            \n",
    "            \n",
    "            '''Find out if same face was detected'''\n",
    "            same_object_detected = False\n",
    "            # For all objects in previous frame \n",
    "            for face_id, pt in self.center_points.items():\n",
    "                \n",
    "                # calculate eucledian distance from previously detected face\n",
    "                dist = math.hypot(cx - pt[0], cy - pt[1])\n",
    "                \n",
    "                # 1st condition: If it is the same object (ditsance < dist_same_obj) update previous objects location to this one's (keeps the object id). \n",
    "                # 2nd condition: But create a NEW object if all faces from previous frame have been matched up already (i.e. we if we've run out of faces to match). \n",
    "                # otherwise youll get the bug where when 2 people show up it is Face 1 and Face 1 as if they are the same face. So current # of objects has to be <= previous number of objects to match to previous objects which makes sense. \n",
    "                if dist < dist_same_obj and counter < len(self.center_points):\n",
    "                    self.center_points[face_id][0] = cx      # update center x\n",
    "                    self.center_points[face_id][1] = cy      # update center y \n",
    "                    \n",
    "                    # Append classification probability to this face's classification history\n",
    "                    frames_to_avg_over = 10\n",
    "                    if len(pt[4]) < frames_to_avg_over:\n",
    "                        self.center_points[face_id][4].append(prob)\n",
    "                        \n",
    "                    # If there are already 10 probabilities in the list then replace one at random (because the code for cycling through them one by one seems too complex in this case)\n",
    "                    else: \n",
    "                        self.center_points[face_id][4][randrange(frames_to_avg_over)] = prob \n",
    "                    \n",
    "                    # Add object info to list to return \n",
    "                    objects_infos.append([xstart, ystart, xend, yend, face_id])\n",
    "                    same_object_detected = True \n",
    "                \n",
    "                    counter+=1\n",
    "                    \n",
    "                    break\n",
    "                \n",
    "            '''New face detected'''\n",
    "            # New object is detected: assign ID to that object\n",
    "            if same_object_detected == False:\n",
    "                self.id_count += 1\n",
    "                self.center_points[self.id_count] = [cx, cy, False, False, [prob]]\n",
    "                \n",
    "                # Add new object to list to return\n",
    "                objects_infos.append([xstart, ystart, xend, yend, self.id_count])\n",
    "            \n",
    "        # Clean the dictionary by center points to remove IDS not used anymore\n",
    "        new_center_points = {}\n",
    "        for object_info in objects_infos:\n",
    "            _, _, _, _, object_id = object_info\n",
    "            center = self.center_points[object_id]\n",
    "            new_center_points[object_id] = center\n",
    "        \n",
    "        # Update dictionary with IDs not used removed\n",
    "        self.center_points = new_center_points.copy()\n",
    "        \n",
    "        # People counter \n",
    "        for face_id, pt in self.center_points.items():\n",
    "            # Note if object has been before the boundary line\n",
    "            if pt[0] > left_boundary_line:\n",
    "                self.center_points[face_id][2] = True\n",
    "            \n",
    "            # Note if the object \n",
    "            # 1. has been detected after the boundary\n",
    "            # 2. has been detected before the boundary (can remove this condition, it is just to ensure that someone who appears from the bottom is not counted) and\n",
    "            # 3. has never been counted before \n",
    "            # Then update it to having been counted and increase people count by 1 \n",
    "            # NB: pt[0] = xcenter, pt[1] = ycenter\n",
    "            if (pt[0] < left_boundary_line) and (pt[2] == True) and pt[3] == False:\n",
    "                self.center_points[face_id][3] = True\n",
    "                self.people_count += 1 \n",
    "                \n",
    "                div = 3 # the larger div is the smaller the perpetrator frame will be \n",
    "                \n",
    "                # labeled count\n",
    "                avg_prob = np.mean(pt[4])\n",
    "\n",
    "                if avg_prob > 0. + uncertain_interval:\n",
    "                    self.mask_count += 1\n",
    "                    \n",
    "                    # TODO: Only for testing, remove \n",
    "                    # Save and display image of masked person\n",
    "                    print(f'MASK ({current_time})')\n",
    "                    detection_data.append(['mask', current_time])\n",
    "                    # Catch exception where face is moving too fast and towards bottom left and perpetrator[0] or perpetrator[1] (width or height or both) ends up being 0.\n",
    "                    perpetrator=frame[0:H, 0:left_boundary_line + W//5]\n",
    "                    perpetrator = cv2.cvtColor(perpetrator, cv2.COLOR_BGR2RGB)\n",
    "                    perpetrator = Image.fromarray(perpetrator, 'RGB')\n",
    "                    plt.imshow(perpetrator)\n",
    "                    plt.show()\n",
    "                    \n",
    "                elif avg_prob < -0. - uncertain_interval:\n",
    "                    self.nomask_count += 1\n",
    "                    \n",
    "                    # Save and display image of unmasked person\n",
    "                    print(f'NO MASK ({current_time})')\n",
    "                    detection_data.append(['no_mask', current_time])\n",
    "                    # Catch exception where face is moving too fast and towards bottom left and perpetrator[0] or perpetrator[1] (width or height or both) ends up being 0.\n",
    "                    perpetrator=frame[0:H, 0:left_boundary_line + W//5]\n",
    "                    perpetrator = cv2.cvtColor(perpetrator, cv2.COLOR_BGR2RGB)\n",
    "                    perpetrator = Image.fromarray(perpetrator, 'RGB')\n",
    "                    plt.imshow(perpetrator)\n",
    "                    plt.show()\n",
    "\n",
    "                else:\n",
    "                    self.uncertain_count += 1\n",
    "\n",
    "                    # Save and display image of potentially unmasked person\n",
    "                    print(f'UNCERTAIN ({current_time})')\n",
    "                    detection_data.append(['uncertain', current_time])\n",
    "                    # Catch exception where face is moving too fast and towards bottom left and perpetrator[1] (the height of the image) ends up being 0. \n",
    "                    perpetrator=frame[0:H, 0:left_boundary_line + W//5]\n",
    "                    perpetrator = cv2.cvtColor(perpetrator, cv2.COLOR_BGR2RGB)\n",
    "                    perpetrator = Image.fromarray(perpetrator, 'RGB')\n",
    "                    plt.imshow(perpetrator)\n",
    "                    plt.show()\n",
    "                                                \n",
    "        return objects_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5acd69-a9ff-40b7-9250-dc633af11b05",
   "metadata": {},
   "source": [
    "### üî¥ 6. Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea060093-7365-4a87-a8da-ed005dd4d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = VideoStream(src=0).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808af80e-a454-408b-b76d-db81a258014a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tracker = TrackerCounter()\n",
    "\n",
    "# Initialise total tracked faces and fps to 0 \n",
    "idd = 0\n",
    "fps_start_time = 0\n",
    "fps = 0\n",
    "\n",
    "# Parameters\n",
    "W = 900\n",
    "left_boundary_line = int(0.35*W)\n",
    "\n",
    "# Store starting time for periodic data extraction\n",
    "start_time = datetime.now()\n",
    "hrs = 1 # every how many hours should the data be exported\n",
    "\n",
    "# initialise csv file with collected data\n",
    "detection_data = []\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "\n",
    "    frame = vs.read()\n",
    "\n",
    "    '''VISUALS'''\n",
    "\n",
    "    # Resize the frame\n",
    "    # NB: cv2.resize lets you choose height and width, imutils.resize only lets you choose width but preserves ratio\n",
    "    # frame = cv2.resize(frame, dsize=(x, y))\n",
    "    frame = imutils.resize(frame, width=W)\n",
    "    H, W = frame.shape[:2] # used in detect_and_classify also\n",
    "\n",
    "    # Calculate ideal font scale\n",
    "    scale = 0.030                      # this value can be from 0 to 1 (0,1] to change the size of the text relative to the image\n",
    "    ideal_font_size = min(W,H)/(25/scale)\n",
    "\n",
    "    # Define box locations\n",
    "    yy = int(ideal_font_size*35)\n",
    "    box1start = np.array((0, 0))\n",
    "    box1end = np.array((int(0.40*W), yy*2))\n",
    "    box2start = np.array((int(0.40*W), 0))\n",
    "    box2end = np.array((W, int(yy*4.5)))\n",
    "\n",
    "    cv2.rectangle(frame, pt1=box1start, pt2=box1end, color=(255, 255, 255), thickness=-1)\n",
    "    cv2.rectangle(frame, pt1=box2start, pt2=box2end, color=(255, 255, 255), thickness=-1)\n",
    "\n",
    "    '''DETECT & CLASSIFY'''\n",
    "    # Detect and Classify for each frame\n",
    "    locs, preds= detect_and_classify(frame, detector, classifier)\n",
    "\n",
    "    '''VISUALS'''\n",
    "\n",
    "    label_probs = [] # This is for tracker_counter to average over multiple classifications\n",
    "    # Live Counter\n",
    "    num_of_masked = 0\n",
    "    num_of_unmasked = 0\n",
    "    num_of_uncertain = 0\n",
    "\n",
    "    # Define classification uncertainty interval\n",
    "    uncertain_interval = 0.2 # 0.5 means 50+% probability of a class for classification. \n",
    "\n",
    "    # loop over face locations and mask predictions\n",
    "    for box, pred in zip(locs, preds):\n",
    "\n",
    "        # unpack the bounding box and predictions\n",
    "        startX, startY, endX, endY = box\n",
    "        mask, no_mask = pred\n",
    "\n",
    "        # 1. Determine the class label 2. Add colour (BGR)\n",
    "        if mask >= 0.5 + uncertain_interval:\n",
    "            label = 'Mask'\n",
    "            colour = (0, 255, 0)\n",
    "            num_of_masked +=1\n",
    "            # for tracker_counter to average over \n",
    "            label_probs.append(mask)\n",
    "\n",
    "        elif no_mask >= 0.5 + uncertain_interval:\n",
    "            label = 'No Mask'\n",
    "            colour = (0, 0, 255)\n",
    "            num_of_unmasked +=1\n",
    "            # for tracker_counter to average over \n",
    "            label_probs.append(-no_mask)\n",
    "\n",
    "        elif (mask >= 0.5) and (mask <= 0.5 + uncertain_interval):\n",
    "            label = 'Uncertain'\n",
    "            colour = (0, 255, 255)\n",
    "            num_of_uncertain +=1\n",
    "            # for tracker_counter to average over \n",
    "            label_probs.append(mask)\n",
    "\n",
    "        elif (no_mask >= 0.5) and (no_mask <= 0.5 + uncertain_interval):\n",
    "            label = 'Uncertain'\n",
    "            colour = (0, 255, 255)\n",
    "            num_of_uncertain +=1\n",
    "            # for tracker_counter to average over \n",
    "            label_probs.append(-no_mask)\n",
    "\n",
    "        # probability and text to display\n",
    "        probability = max(mask, no_mask) * 100\n",
    "        label_text = f'{label}: {probability:.1f}%'\n",
    "\n",
    "        # 1. Display label  \n",
    "        cv2.putText(img=frame, text=label_text, org=(startX, startY - 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=colour, thickness=2)\n",
    "        # 2. Display bounding box\n",
    "        cv2.rectangle(img=frame, pt1=(startX, startY), pt2=(endX, endY), color=colour, thickness=2)\n",
    "\n",
    "    '''TRACK & COUNT'''\n",
    "\n",
    "    # objects_info = [object1_info, object2_info, ...] where object_info = [Xstart, Ystart, Xend, Yend, id_of_object]\n",
    "    objects_info = tracker.update(locs, label_probs, W, H, dist_same_obj=(W + H / 14))\n",
    "\n",
    "    '''VISUALS'''  \n",
    "\n",
    "    # for all objects\n",
    "    for object_info in objects_info: \n",
    "        Xstart, Ystart, Xend, Yend, idd = object_info\n",
    "\n",
    "        # ID of face\n",
    "        cv2.putText(img=frame, text=f'Face {idd}', org=(Xstart, Ystart-40), fontScale=1.4, fontFace=cv2.FONT_HERSHEY_PLAIN, color=(155, 149, 24), thickness=2)\n",
    "\n",
    "    # Calculate fps\n",
    "    fps_end_time = time.time()\n",
    "    time_diff = fps_end_time - fps_start_time\n",
    "    fps = int(1/time_diff)\n",
    "    fps = f'FPS: {fps}'\n",
    "    fps_start_time = fps_end_time\n",
    "\n",
    "    # Calculate current time and export data\n",
    "    current_time = datetime.now()\n",
    "    current_time_str = str(current_time)[:-7]\n",
    "    current_time_export = current_time_str.replace(':', '.')\n",
    "    time_difference = (current_time - start_time).seconds/3600\n",
    "\n",
    "    if  time_difference >= hrs:\n",
    "\n",
    "        # Export csv and summarise results\n",
    "        detection_data_exp = pd.DataFrame(data=detection_data, columns=['Label', 'Datetime'])\n",
    "        detection_data_exp.to_csv(f'{current_time_export}_detection_data.csv', index=False)\n",
    "\n",
    "        # Unburden memory\n",
    "        detection_data = []\n",
    "\n",
    "        # Summarise and visualise results\n",
    "        num_of_people = detection_data_exp.shape[0]\n",
    "        if num_of_people > 0:\n",
    "            mask = detection_data_exp[detection_data_exp.Label == 'mask'].shape[0]\n",
    "            no_mask = detection_data_exp[detection_data_exp.Label == 'no_mask'].shape[0]\n",
    "            uncertain = detection_data_exp[detection_data_exp.Label == 'uncertain'].shape[0]\n",
    "\n",
    "            print(f'{num_of_people} people.')\n",
    "            print(f'{mask} mask.')\n",
    "            print(f'{no_mask} no_mask.')\n",
    "            print(f'{uncertain} uncertain.')\n",
    "\n",
    "            # Export Pie Chart\n",
    "            dpi=110\n",
    "            fig = plt.figure(figsize=(8, 6), dpi=dpi)\n",
    "\n",
    "            if uncertain != 0:\n",
    "                plt.pie(x=[mask, no_mask, uncertain], labels=['Mask', 'No Mask', 'Uncertain'], colors=['green', 'red', 'yellow'], startangle=90, autopct='%1.1f%%', textprops={'fontsize': 14})\n",
    "            if uncertain == 0: \n",
    "                plt.pie(x=[mask, no_mask], labels=['Mask', 'No Mask'], colors=['green', 'red'], startangle=90, autopct='%1.1f%%', textprops={'fontsize': 14})\n",
    "\n",
    "            plt.title(f'Total People: {num_of_people}', fontweight='bold', fontsize=15)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            fig.savefig(f'{current_time_export}_face_covering_pie_chart.png', dpi = dpi)\n",
    "\n",
    "        else:\n",
    "            print('No people detected.')\n",
    "\n",
    "        # reset time\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "    # VISUALS: Display FPS and Current Time\n",
    "    cv2.putText(img=frame, text=current_time_str, org=(box1start[0] + box1start[0]//14, yy), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=ideal_font_size, color=(0, 0, 0), thickness=2)\n",
    "    cv2.putText(img=frame, text=fps, org=(box1start[0] + box1start[0]//14, int(yy*1.8)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=ideal_font_size, color=(0, 0, 0), thickness=2)\n",
    "\n",
    "    # VISUALS: Display boundary line\n",
    "    cv2.line(img=frame, pt1=(left_boundary_line, 0), pt2=(left_boundary_line, H), color=(45, 174, 102), thickness=5) \n",
    "\n",
    "    # VISUALS: Display live people counter \n",
    "    cv2.putText(img=frame, text=f'People Count: {tracker.people_count}', org=(box2start[0] + box2start[0]//15, yy), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=ideal_font_size, color=(0, 0, 0), thickness=2)\n",
    "    percent_masked = 0\n",
    "    percent_unmasked = 0\n",
    "    percent_uncertain = 0\n",
    "    if tracker.people_count != 0:\n",
    "        percent_masked = np.round(tracker.mask_count / tracker.people_count * 100, 1) \n",
    "        percent_unmasked = np.round(tracker.nomask_count / tracker.people_count * 100, 1) \n",
    "        percent_uncertain = np.round(tracker.uncertain_count / tracker.people_count * 100, 1) \n",
    "    cv2.putText(img=frame, text=f'Masked:       {tracker.mask_count} ({percent_masked}%)', org=(box2start[0] + box2start[0]//15, yy*2), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=ideal_font_size, color=(0, 155, 0), thickness=2)\n",
    "    cv2.putText(img=frame, text=f'Unmasked:    {tracker.nomask_count} ({percent_unmasked}%)', org=(box2start[0] + box2start[0]//15, int(yy*3)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=ideal_font_size, color=(0, 0, 155), thickness=2)\n",
    "    cv2.putText(img=frame, text=f'Uncertain:     {tracker.uncertain_count} ({percent_uncertain}%)', org=(box2start[0] + box2start[0]//15, int(yy*4)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=ideal_font_size, color=(0, 155, 155), thickness=2)\n",
    "\n",
    "    '''END STREAM'''\n",
    "\n",
    "    # Show the output frame in real-time\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Terminate if `q` is pressed. waitKey(0): keeps image still until a key is pressed. waitKey(x) it will wait x miliseconds each frame\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "        \n",
    "\n",
    "'''EXPORT RESULTS'''\n",
    "\n",
    "#Export csv file and summarise results\n",
    "detection_data_exp = pd.DataFrame(data=detection_data, columns=['Label', 'Datetime'])\n",
    "detection_data_exp.to_csv(f'{current_time_export}_detection_data.csv', index=False)\n",
    "\n",
    "# Unburden memory\n",
    "detection_data = []\n",
    "\n",
    "# Summarise and visualise results\n",
    "num_of_people = detection_data_exp.shape[0]\n",
    "if num_of_people > 0:\n",
    "    mask = detection_data_exp[detection_data_exp.Label == 'mask'].shape[0]\n",
    "    no_mask = detection_data_exp[detection_data_exp.Label == 'no_mask'].shape[0]\n",
    "    uncertain = detection_data_exp[detection_data_exp.Label == 'uncertain'].shape[0]\n",
    "\n",
    "    print(f'{num_of_people} people.')\n",
    "    print(f'{mask} mask.')\n",
    "    print(f'{no_mask} no_mask.')\n",
    "    print(f'{uncertain} uncertain.')\n",
    "\n",
    "    # PIE CHART\n",
    "    dpi=110\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=dpi)\n",
    "\n",
    "    if uncertain != 0:\n",
    "        plt.pie(x=[mask, no_mask, uncertain], labels=['Mask', 'No Mask', 'Uncertain'], colors=['green', 'red', 'yellow'], startangle=90, autopct='%1.1f%%', textprops={'fontsize': 14})\n",
    "    if uncertain == 0: \n",
    "        plt.pie(x=[mask, no_mask], labels=['Mask', 'No Mask'], colors=['green', 'red'], startangle=90, autopct='%1.1f%%', textprops={'fontsize': 14})\n",
    "\n",
    "    plt.title(f'Total People: {num_of_people}', fontweight='bold', fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig(f'{current_time_export}_face_covering_pie_chart.png', dpi = dpi)\n",
    "\n",
    "else:\n",
    "    print('No people detected.')\n",
    "\n",
    "\n",
    "# Cleanup\n",
    "vs.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187f4d2-1629-4424-8f03-51beb93a027c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
